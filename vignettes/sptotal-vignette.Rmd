---
title: "sptotal Package"
author:
  - Matthew Higham, Jay M.Ver Hoef, Bryce M. Frank, Francisco M. Gutierrez 
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, echo = FALSE, message = FALSE}
########################################################################
########################################################################
########################################################################
#        Notation
########################################################################
########################################################################
########################################################################
```

# Introduction

The `sptotal` package was developed for estimating (predicting) a mean or total from a finite number of sample units in a fixed geographic area. Estimating totals and means from a finite population is an important goal for both academic research and management of environmental data. One naturally turns to classical sampling methods, such as simple random sampling (SRS) when confronted with these goals. Classical sampling methods depend on design-based probability and are robust. Very few assumptions are required because the probability distribution for inference comes from the sample design, which is known and under our control.  For design-based methods, sample plots are chosen at random, they are measured or counted, and inference is obtained from the probability of sampling those units randomly based on some design (e.g., Horwitz-Thompson estimation). However, as an alternative, we will use model-based methods, specifically geostatistics, to accomplish the same goals. When using geostatistics, we assume the data were produced by a stochastic process with parameters that can be estimated.  The relevant theory is given by Ver Hoef (2008), and the `sptotal` package is designed to make this easy for you.

In the sptotal package, our goal is to estimate some linear function of all of the sample units, call it $\tau(\mathbf{z}) = \mathbf{b}^\prime \mathbf{z}$, where $\mathbf{z}$ is a vector of the realized values for all the sample units and $\mathbf{b}$ is a vector of weights. By "realized," we mean that whatever processes produced the data have already happened, and that, if we had enough resources, we could measure them all, obtaining a complete census. For example, the function $\tau(\mathbf{z})$ could be the population mean.  In that case, if there are $N$ samples in the whole population, then every element of $\mathbf{b}$ constains $1/N$. If $\tau(\mathbf{z})$ is a population total, then every element of $\mathbf{b}$ contains a $1$.  We often want to obtain the mean or total from a subset of sample units, called small area estimation.  For example, if we want a total from a small area, then for each sample in that area, $\mathbf{b}$ contains a $1$, otherwise $\mathbf{b}$ contains a $0$. The key idea is that $\mathbf{b} can contain any set of weights that we would like to multiply times each value in a population, and then these are summed, yielding a weighted sum. 

The vector $\mathbf{b}$ contains the weights that we would apply if we could measure or count every observation, but, because of cost consideration, we usually only have a sample. Let us divide our population into those that we sampled (measured or counted), denoted $\mathbf{z}_s$, and those that are unsampled, denoted $\mathbf{z}_u$

## Statistical Background

There are some problems with design-based methods. Because few assumptions are required, they may lack power in cases where further assumptions are justified. This appears to be especially true in the case of ''small area'' estimation. The term small area estimation refers to making an inference on a smaller geographic area within the overall study area. There may be few or no samples within that small area, so that estimation may not be possible or variances become exceedingly large. An alternative is to assume that the data were generated by a stochastic process and use model-based approaches (see, e.g., Fay and Harriot 1979, Ghosh and Meeden 1986, and Prasad and Rao 1990, Ver Hoef 2008). 

It is assumed that $\mathbf{z}$ is a realization of a spatial stochastic process. Geostatistical models and methods are used (for a review, see Cressie, 1993). Geostatistics has been developed for point samples. If the samples are very small, an infinite population is assumed. The average value over any aggregated area can be predicted using methods such as block kriging. Thus it appears that this is closely related to small area estimation, but where samples come from point locations rather than a finite set of sample units. While there is a large literature on geostatistics and block kriging methods, they have been developed for infinite populations. This package is designed for the case where we have a finite collection of plots and we assume that the data were produced by a spatial stochastic process. Detailed developements are given in Ver Hoef (2008), and we give an overview here.

# Data 

Prior to using the `sptotal` package, you need to become familiar with getting your data in the proper format.  This is very simple, as we use the `data.frame` in `R', and we have tried to make this easy and give you a variety of tools depending on how you have created and stored your data.

## Data Frame Structure

Data input for the `sptotal` package is a `data.frame`.  In the next section, we will show you how to work the spatial objects in the `sp` class.  The basic information required to fit a spatial linear model, and make predictions, are the response variable, covariates, the x- and y-coordinates, and a column of weights.  You can envision your whole population of possible samples as a `data.frame` organized as follows,

```{r, echo = FALSE, message = FALSE, fig.align="center", cache = TRUE}
par(mar = c(0,0,0,0))
plot(c(0,1), c(0,1), type = 'n', bty = 'n', xaxt = 'n', yaxt = 'n', 
  xlab = '', ylab = '')
rect(0, .7, .08, 1, col = rgb(228/255,26/255,28/255))
rect(0, 0, .08, .68, col = 'white')
rect(.1, .7, .48, 1, col = rgb(55/255,126/255,184/255))
rect(.1, 0, .48, .68, col = rgb(55/255,126/255,184/255, alpha = .3))
rect(.5, .7, .58, 1, col = rgb(77/255,175/255,74/255))
rect(.6, .7, .68, 1, col = rgb(77/255,175/255,74/255))
rect(.5, 0, .58, .68, col = rgb(77/255,175/255,74/255, alpha = .3))
rect(.6, 0, .68, .68, col = rgb(77/255,175/255,74/255, alpha = .3))
rect(.7, .7, .78, 1, col = rgb(152/255,78/255,163/255))
rect(.7, 0, .78, .68, col = rgb(152/255,78/255,163/255, alpha = .3))
rect(.8, .7, 2, 1, col = rgb(255/255,127/255,0/255))
rect(.8, 0, 2, .68, col = rgb(255/255,127/255,0/255, alpha = .3))
```
where the red rectangle represents the column of the response variable, and the top part, colored in red, are observed locations, and the lower part, colored in white, are the unobserved values.  To the right, colored in blue, are possibly several columns containing covariates thought to be _predictive_ for the response value at each location.  Covariates must be known for both observed and unobserved locations, and the covariates for unobserved locations are shown as pale blue below the darker blue covariates for observed locations above.  The `data.frame` must have x- and y-coordinates, and they are showns as two columns colored in green, with the coordinates for the unobserved locations shown as pale green below the darker green coordinates for the observed locations above. The `data.frame` can have a column of weights.  If one is not provided, we assume a column of all ones.  The column of weights is purple, with weights for the observed locations a darker shade, above the lighter shade of purple representing weights for unsampled locations.  Finally, your `data.frame` may contain columns that are not relevant to predicting the weighted sum.  These columns are represented by the orange color, with the sampled locations a darker shade, above the unsampled locations with the ligher shade.

Of course, the data do not have to be in exactly this order, either in terms of rows or columns.  Sampled and unsampled rows can be intermingled, and columns of response variable, covariates, coordinates, and weights can be also be intermingled.  The figure above is an idealized graphic of the data.  However, it helps envision how the data are used, and illustrate the goal.  We desire a weighted sum, where the weights (in the purple column) are multiplied times the response variable (red/white) column, and then summed.  Because some of the response values are unknown (the white values in the response column), covariates and spatial information (obtained from the x- and y-coordinates) are used to _predict_ the unobserved (white) values.  The weights (purple) are then applied to both the observed response values (red), and the predicted response values (white), to obtain a weighted sum.  Because we use predictions for unobserved response values, it is important to assess our uncertainty, and the software provides both an estimate of the weighted sum for the response variable, and also its estimated variance.  


## Working with Spatial Coordinate Systems

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
