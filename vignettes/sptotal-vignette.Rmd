---
title: "Using the sptotal Package -- Inference for Weighted Sums from Finite Spatial Populations"
author:
  - Matthew Higham, Jay M.Ver Hoef, Bryce M. Frank, Francisco M. Gutierrez 
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
vignette: >
  %\VignetteIndexEntry{Using the sptotal Package -- Inference for Weighted Sums from Finite Spatial Populations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
```{css, echo = FALSE}
caption, .caption{
  font-style:italic;
  margin-top:0.5em;
  margin-bottom:0.5em;
  width:99%;
  text-align: left;
}
body{
  font-size: 12pt;
}
p {line-height: 1.7;}
tr:nth-child(even) {background-color: #f2f2f2;}
th {
    background-color: #4CAF50;
    color: black;
}
p {
padding-top: 7px;
padding-bottom: 7px;
}

```

```{r, echo = FALSE, message = FALSE}
########################################################################
########################################################################
########################################################################
#        Introduction
########################################################################
########################################################################
########################################################################
```

# Introduction

The `sptotal` package was developed for estimating (predicting) a mean or total from a finite number of sample units in a fixed geographic area. Estimating totals and means from a finite population is an important goal for both academic research and management of environmental data. One naturally turns to classical sampling methods, such as simple random sampling (SRS) when confronted with these goals. Classical sampling methods depend on design-based probability and are robust. Very few assumptions are required because the probability distribution for inference comes from the sample design, which is known and under our control.  For design-based methods, sample plots are chosen at random, they are measured or counted, and inference is obtained from the probability of sampling those units randomly based on some design (e.g., Horwitz-Thompson estimation). However, as an alternative, we will use model-based methods, specifically geostatistics, to accomplish the same goals. When using geostatistics, we assume the data were produced by a stochastic process with parameters that can be estimated.  The relevant theory is given by Ver Hoef (2008), and the `sptotal` package is designed to make this easy for you.

In the sptotal package, our goal is to estimate some linear function of all of the sample units, call it $\tau(\mathbf{z}) = \mathbf{b}^\prime \mathbf{z}$, where $\mathbf{z}$ is a vector of the realized values for all the sample units and $\mathbf{b}$ is a vector of weights. By "realized," we mean that whatever processes produced the data have already happened, and that, if we had enough resources, we could measure them all, obtaining a complete census. For example, the function $\tau(\mathbf{z})$ could be the population mean. In that case, if there are $N$ samples in the whole population, then every element of $\mathbf{b}$ contains $1/N$. If $\tau(\mathbf{z})$ is a population total, then every element of $\mathbf{b}$ contains a $1$. We often want to obtain the mean or total from a subset of sample units, called small area estimation.  For example, if we want a total from a small area, then for each sample in that area, $\mathbf{b}$ contains a $1$, otherwise $\mathbf{b}$ contains a $0$. The key idea is that $\mathbf{b} can contain any set of weights that we would like to multiply times each value in a population, and then these are summed, yielding a weighted sum. 

The vector $\mathbf{b}$ contains the weights that we would apply if we could measure or count every observation, but, because of cost consideration, we usually only have a sample. Let us divide our population into those that we sampled (measured or counted), denoted $\mathbf{z}_s$, and those that are unsampled, denoted $\mathbf{z}_u$.

```{r, echo = FALSE, message = FALSE}
########################################################################
########################################################################
########################################################################
#        Data
########################################################################
########################################################################
########################################################################
```

# Data 

Prior to using the `sptotal` package, the data needs to be in `R` in the proper format. For this package, we assume that your data set is a `data.frame()` object, described below.

## Data Frame Structure

Data input for the `sptotal` package is a `data.frame`.  In the next section, we will show you how to work the spatial objects in the `sp` class.  The basic information required to fit a spatial linear model, and make predictions, are the response variable, covariates, the x- and y-coordinates, and a column of weights.  You can envision your whole population of possible samples as a `data.frame` organized as follows,

```{r, echo = FALSE, message = FALSE, fig.align="center", cache = FALSE}
par(mar = c(0,0,0,0))
plot(c(0,1), c(0,1), type = 'n', bty = 'n', xaxt = 'n', yaxt = 'n', 
  xlab = '', ylab = '')
rect(0, .7, .08, 1, col = rgb(228/255,26/255,28/255))
rect(0, 0, .08, .68, col = 'white')
rect(.1, .7, .48, 1, col = rgb(55/255,126/255,184/255))
rect(.1, 0, .48, .68, col = rgb(55/255,126/255,184/255, alpha = .3))
rect(.5, .7, .58, 1, col = rgb(77/255,175/255,74/255))
rect(.6, .7, .68, 1, col = rgb(77/255,175/255,74/255))
rect(.5, 0, .58, .68, col = rgb(77/255,175/255,74/255, alpha = .3))
rect(.6, 0, .68, .68, col = rgb(77/255,175/255,74/255, alpha = .3))
rect(.7, .7, .78, 1, col = rgb(152/255,78/255,163/255))
rect(.7, 0, .78, .68, col = rgb(152/255,78/255,163/255, alpha = .3))
rect(.8, .7, 2, 1, col = rgb(255/255,127/255,0/255))
rect(.8, 0, 2, .68, col = rgb(255/255,127/255,0/255, alpha = .3))
```

where the red rectangle represents the column of the response variable, and the top part, colored in red, are observed locations, and the lower part, colored in white, are the unobserved values. To the right, colored in blue, are possibly several columns containing covariates thought to be _predictive_ for the response value at each location. Covariates must be known for both observed and unobserved locations, and the covariates for unobserved locations are shown as pale blue below the darker blue covariates for observed locations above.  The `data.frame` must have x- and y-coordinates, and they are showns as two columns colored in green, with the coordinates for the unobserved locations shown as pale green below the darker green coordinates for the observed locations above. The `data.frame` can have a column of weights.  If one is not provided, we assume a column of all ones. The column of weights is purple, with weights for the observed locations a darker shade, above the lighter shade of purple representing weights for unsampled locations. Finally, the `data.frame` may contain columns that are not relevant to predicting the weighted sum.  These columns are represented by the orange color, with the sampled locations a darker shade, above the unsampled locations with the ligher shade.

Of course, the data do not have to be in exactly this order, either in terms of rows or columns.  Sampled and unsampled rows can be intermingled, and columns of response variable, covariates, coordinates, and weights can be also be intermingled. The figure above is an idealized graphic of the data. However, the "pretty" graph helps envision how the data are used and illustrate the goal. We desire a weighted sum, where the weights (in the purple column) are multiplied times the response variable (red/white) column, and then summed.  Because some of the response values are unknown (the white values in the response column), covariates and spatial information (obtained from the x- and y-coordinates) are used to _predict_ the unobserved (white) values. The weights (purple) are then applied to both the observed response values (red), and the predicted response values (white), to obtain a weighted sum. Because we use predictions for unobserved response values, it is important to assess our uncertainty, and the software provides both an estimate of the weighted sum for the response variable as well as its estimated prediction variance.  

To demonstrate the package, we created some simulated data so they are perfectly behaved, and we know exactly how they were produced. Here, we give a brief description before using the main features of the package. To get started, type

```{r}
library(sptotal)
```

Type 

```{r}
data(simdata)
```

and then `simdata` sill be available in your workspace.  To see the structure of `simdata`, type

```{r}
str(simdata)
```

Here, we see that `simdata` is a data.frame with 400 records. The spatial coordinates are `numeric` variables in columns named `x` and `y`.  We created 7 continuous covariates, `X1` through `X5`.  The variables `X1` through `X5` were all created using the `rnorm()` function, so they are all standard normal variates that are independent between and within variable.  Variables `X6` and `X7` were independent from each other, but spatially autocorrelated within, each with a variance parameter of 1, an autocorrelation range parameter of 0.2 from an exponential model, and a small nugget effect of 0.01.  The variables `F1` and `F2` are factor variables with 3 and 5 levels, respectively. The variable `Z` is the response. Data were simulated from the model

\begin{align*}
Z_i = 10 + &0 \cdot X1_i + 0.1 \cdot X2_i + 0.2 \cdot X3_i + 0.3 \cdot X4_i + 0.4 \cdot X5_i + \\
    &0.4 \cdot X6_i + 0.1 \cdot X7_i + F1_i + F2_i + \delta_i + \varepsilon_i
\end{align*}

where factor levels for `F1` have effects $0, 0.4, 0.8$, and factor levels for `F2` have effects $0, 0.1, 0.2, 0.3, 0.4$. The random errors $\{\delta_i\}$ are spatially autocorrelated from an exponential model,

$$
\textrm{cov}(\delta_i,\delta_j) = 2*\exp(-d_{i,j})
$$  

where $d_{i,j}$ is Euclidean distance between locations $i$ and $j$.  In geostatistics terminology, this model has a partial sill of 2 and a range of 1. The random errors $\{\varepsilon_i\}$ are independent with variance 0.02, and this variance is called the nugget effect. Two columns with weights are included, `wts1` contains 1/200 for each row, so the weighted sum will yield a prediction of the overall mean. The column `wts2` contains a 1 for 25 locations, and 0 elsewhere, so the weighted sum will be a prediction of a total in the subset of 25 locations.

The spatial locations of `simdata` are in a $20 \times 20$ grid uniformly spaced in a box with sides of length 1,

```{r, fig.width = 5, fig.height = 5, fig.align = "center", cache = FALSE}
require(ggplot2)
ggplot(data = simdata, aes(x = x, y = y)) + geom_point() +
  geom_point(data = subset(simdata, wts2 == 1), colour = "red")
```

The locations of the 25 sites where `wts2` is equal to one, are shown in red.

We have simulated the data for the whole population.  This is convenient, because we know the true means and totals.  However, we will now sample from this population to provide a more realistic setting where we can measure only a part of the whole population.  In order to make results reproducible, we use the `set.seed` command, along with `sample`. In order to understand how to use the package, it's not necessary to understand the following code chunk. The code below simply replaces some of the response values with `NA` for missing.

```{r}
set.seed(1)
# take a random sample of 100
obsID <- sample(1:nrow(simdata), 100)
simobs <- simdata
simobs$Z <- NA
simobs[obsID,'Z'] <- simdata[obsID, 'Z']
```

We now have a data set where the whole population is known, `simdata`, and another one, `simobs`, where 75% of the population has been replaced by `NA`.  Next we show the sampled sites solid circles, while the missing values are shown as open circles, and we use red again to show sampled site within the small area of 25 locations.

```{r, fig.width = 5, fig.height = 5, fig.align = "center", cache = FALSE}
ggplot(data = simobs, aes(x = x, y = y)) + geom_point(shape = 1) +
  geom_point(data = subset(simobs, !is.na(Z)), shape = 16) +
  geom_point(data = subset(simobs, !is.na(Z) & wts2 == 1), shape = 16,
    colour = "red")
```

We will use the `simobs` data to illustrate use of the `sptotal` package.

```{r, echo = FALSE, message = FALSE}
########################################################################
########################################################################
########################################################################
#        Using the sptotal package
########################################################################
########################################################################
########################################################################
```

# Using the `sptotal` Package

After data preparation, using the `sptotal` package occurs in two primary stages. In the first, we fit a spatial linear model. This stage estimates spatial regression coefficients and spatial autocorrelation parameters. In the second stage, we predict the missing values for the response value, and create a prediction for the weighted sum of all response variable values, both observed and predicted. To show how the package works, we demonstrate on ideal, simulated data. Then, we give a realistic example to provide further insight and documentation. The realistic example also has a section on data preparation steps.

## Fitting a Spatial Linear Model

We continue with our use of the simulated data, `simobs`, to illustrate fitting the spatial linear model. The spatial model-fitting function is `slmfit` (spatial-linear-model-fit), which uses a formula like many other model-fitting functions in `R` (e.g., the `lm()` function). To fit a basic spatial linear model we use

```{r}
slmfit_out1 = slmfit(formula = Z ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + F1 + F2, 
  data = simobs, xcoordcol = 'x', ycoordcol = 'y',
  CorModel = "Exponential")
```

The documentation describes the arguments in more detail, but as mentioned earlier, the linear model includes a formula argument, and the `data.frame` that is being used as a data set. We also need to include which columns contain the $x$- and $y$-coordinates, which are arguments `xcoordcol` and `ycoordcol`, respectively. In the above example, we specify `'x'` and `'y'` as the column coordinates arguments since the names of the coordinate columns in our simulated data set are `'x'` and `'y'`. We also need to specify which spatial autocorrelation model that is fitted, which is given by the `CorModel` argument. As with many other linear model fits, we can obtain a summary of the model fit,

```{r}
summary(slmfit_out1) 
```

The output looks similar to the `summary` of a standard `lm` object, but there is some extra output at the end that gives our fitted covariance parameters. 

## Prediction

After we have obtained a fitted spatail linear model, we can use the `predict.slmfit` function to construct a data frame of predictions for the unsampled sites. By default, the `predict` function assumes that we are predicting the population total and outputs this predicted total, the prediction variance for the total, as well as the original data frame input into `slmfit` appended with site-by-site predictions and site-by-site prediction variances. We name this object `pred_obj` in the chunk below.

```{r, results = "hide"}
pred_obj <- predict(slmfit_out1)
```

The output of the `predict` function is too lengthy to fit on this page and can be a bit cumbersome to read anyway. To obtain useful output, the user has a couple of choices. If the user is familiar with `R`, then he or she might want to construct his or her own summary output, maps, variograms, etc. One of the outputs of `predict` is a data frame that is appended with the site by site predictions that would facilitate construction of, for example, a map with predictions. This is particularly useful if we have a shapefile.

```{r}
prediction_df <- pred_obj$Pred_df
```

Alternatively, the user could take advantage of some pre-specified package output functions `check.variogram`, `get.predinfo`, and `get.predplot`. 

First, `check.variogram` constructs an empirical variogram of the model residuals and overlays the REML-fitted variogram model with a line:

```{r}
check.variogram(slmfit_out1)
```

Note that the fitted line may not appear to fit the empirical variogram perfectly for a couple of reasons. First, only pairs of points that have a distance between 0 and one-half the maximum distance are shown. Second, the fitted model is estimated using REML, which may give different results than using weighted least squares.

If we are satisfied with the fit of the variogram, we can examine our prediction and obtain a confidence interval for this prediction using `get.predinfo`. While the `check.variogram` function takes the fitted spatial linear model as input, the `get.predinfo` function uses the object from the `predict` function, which we previously named `pred_obj`.

```{r}
get.predinfo(pred_obj)
```

Finally, to get a basic "map" of the predictions, we can use the `get.predplot` function. 

```{r}
get.predplot(pred_obj)
```

The map shows the distribution of counts across sampled and unsampled sites. Its purpose is simply to give the user a quick idea of the distribution of counts. However, using the prediction data frame generated from the `predict` function, the user can use `ggplot2` or any other plotting package to construct their own map that may be more useful in their context.

#####Comparison to Other Methods

We can now compare `sptotal` to a few other methods that could be used to predict the population total. Note that, this is a simple comparison using a single simulated data set, not a full simulation study, which is beyond the scope of this vignette. The purpose is just to show how the functions in `sptotal` compare to methods that the reader may be more familiar with.

First, we compare the predicted total to what would be predicted under a simple random sampling estimator. To obtain a simple random sampling estimator, we only need the sample size,............

```{r}
n <- sum(!is.na(simobs$Z))
N <- nrow(simobs)
ybar <- mean(simobs$Z, na.rm = TRUE)
(N - n) * ybar + sum(simobs$Z, na.rm = TRUE)
sqrt((sd(simobs$Z, na.rm = TRUE) ^ 2 * n) * (N - n) / n)
```

#####Prediction for a Quantity Other Than the Total

If we want to predict a quantity other than the population total, then we need to specify the column in our data set that has the appropriate prediction weights in a `wtscol` argument. For example, if we want to predict the total for the 25 sites in coloured in red, then we can use

```{r}
pred_obj2 <- predict(slmfit_out1, wtscol = "wts2")
```

## Moose Abundance from Aerial Surveys

For an example with real data, we first consider a data set on moose abundance in Alaska. Begin by loading the data into `R`. Unlike the simulated data, `AKmoose` is an `sp` object. In order to use the functions in this package, we need to extract the coordinates and relevant data from the `sp` object and put this information into a rectangular data frame. The easy part is getting the data; the more difficult part is getting the coordinates into a usable form. 

##### Issue 1: Dealing with Spatial Coordinates

Our goal is to append the x and y-coordinates to the data frame with the survey data. Specifically, we want the _centroid_ coordinates for each site. We first want to convert the projection into standard Latitude/Longitude coordinates.

```{r}
require(rgeos); require(sp)
AKmoose@proj4string ## note that the projection is __not__ currently in lat/lon

latlon <- sp::spTransform(AKmoose, CRS("+init=epsg:4326")) 
## 4326 is the code for conversion into the standard lat/lon system
```

Next, we want to get the centroids of each of the sites and combine the centroids with the survey data. To obtain centroids, we use the `rgeos` package.

ADD rgeos to package dependencies/options

```{r}
centroids <- data.frame(ID = latlon@data,
  x = rgeos::gCentroid(latlon, byid=TRUE)@coords[ ,'x'],
  y = rgeos::gCentroid(latlon, byid=TRUE)@coords[ ,'y'])
```

Finally, for most spatial prediction, we want to use a trans-mercator projection instead of latitude-longitude so that physical distance between two sites is accurately represented. The `LLtoUTM()` function in this package provides a convenient way to convert latitude/longitude coordinates into user-defined transmercator coordinates. 

```{r}
xy <- LLtoUTM(mean(centroids$x), centroids$y, centroids$x)$xy
```

Finally, we add the trans-mercator coordinates to our data frame with the survey data. We first extract the survey data from our `sp` object using

```{r}
data(AKmoose) ## load the data set into the package
moose_df <- AKmoose@data ## name the data set moose_df
head(moose_df) ## look at the first 6 observations
```

We see that, in addition to the `total` column, which has counts of moose, the data set also has `strat`, a covariate that is either `L` for Low or `M` for medium, and `surveyed`, which is a `0` if the site wasn't sampled and a `1` if the site was sampled. 

And then the trans-mercator coordinates can be added to the survey data frame.

```{r}
moose_df$x = xy[ ,'x']
moose_df$y = xy[ ,'y']
```

It might be helpful to compare the lat/lon coordinates of the original data frame to the TM coordinates in the new data frame to make sure that we did the transformation correctly.

```{r, results = "hide"}
cbind(moose_df$x, moose_df$y, centroids$x, centroids$y)
```

Now, the `moose_df` data frame is in a more workable form for the `sptotal` package. However, there are still a couple of issues involving how the count data is stored and which sites were sampled that may be somewhat common in real data sets, which we address next.

##### Issue 2: Dealing with Count Vector Specifications

Let's look specifically at the counts in this moose data set in the `total` column:

```{r}
head(moose_df)
str(moose_df$total)
```

The first issue is that our original `sp` object had `total` as a factor, which `R` treats as a categorical variable. `total` should be numeric, and, in fact, the variable `surveyed` has the same issue. If we were to keep `total` as a factor and try to run `slmfit`, we would get a convenient error message, reminding us to make sure that our response variable is numeric, not a factor or character:

```{r, error = TRUE}
slmfit_out_moose <- slmfit(formula = total ~ strat, 
  data = moose_df, xcoordcol = 'x', ycoordcol = 'y',
  CorModel = "Exponential")
```

We first want to convert these two columns into numeric variables instead of factors. There are packages that can help with this conversion, like `dplyr` and `forcats`, but we opt for base `R` functions here.

```{r}
moose_df$surveyed <- as.numeric(levels(moose_df$surveyed))[moose_df$surveyed]
moose_df$total <- as.numeric(levels(moose_df$total))[moose_df$total]
```

This may not be an issue with the data frame you are working with, in which case you can ignore the above code. The `str()` command will tell you whether your variables are coded as factors or numeric.

After conversion to numeric variables, note that the first 6 observations for the `total` variable are all 0, but, the first two sites and the fourth, fifth, and sixth sites weren't actually sampled. Without some modification to this variable, `sptotal` wouldn't be able to differentiate between zeroes that were zero due to a site really having 0 counts or 0 density at the site and zeroes that were zero due to the site not being sampled. The following code converts the `total` variable on sites that were __not__ surveyed (`surveyed` = `0`) to `NA`.

```{r}
moose_df$total[moose_df$surveyed == 0] <- NA
head(moose_df)
```

Now that

* we have x and y coordinates in TM format,

* our response variable is numeric and not a factor, and

* the column with our counts has `NA` values for sites that were not surveyed,

we can proceed to use the functions in `sptotal` in a similar way to how the functions were used for the simulated data.

```{r}
slmfit_out_moose <- slmfit(formula = total ~ strat, 
  data = moose_df, xcoordcol = 'x', ycoordcol = 'y',
  CorModel = "Exponential")
summary(slmfit_out_moose)
check.variogram(slmfit_out_moose)

pred_moose <- predict(slmfit_out_moose)
get.predinfo(pred_moose)
get.predplot(pred_moose)
```

We obtain a predicted total of 1596 animals with 90% lower and upper confidence bounds of 921 and 2271 animals, respectively. Unlike the simulation setting, there is no "true total" we can compare our prediction to, because, in reality, not all sites were sampled!

## Tree Biomass from Forest Inventories

As a second example, consider prediction of the total biomass for a region in Oregon. For this data set, we have biomass density at 2788 sites throughout Oregon. While we could make predictions at other sites, for this analysis, we suppose that we had only sampled 788 of the 2788 sites and predict biomass at the 2000 "unsampled" sites. In doing so, we can check our predicted average biomass with what is actually observed.

Having trouble thinking about a total for this without having a variable for area of each site....unless all sites have equal areas.

Having the same issue with the US Lakes example....it's difficult to think of anything useful when the lakes have vastly different volumes.


# Statistical Background

Spatial prediction can be used to estimate means and totals over geographic regions, and also the special case of ''small area'' estimation. The term small area estimation refers to making an inference on a smaller geographic area within the overall study area. There may be few or no samples within that small area, so that estimation by classical sampling methods may not be possible or variances become exceedingly large. An alternative is to assume that the data were generated by a stochastic process and use model-based approaches (see, e.g., Fay and Harriot 1979, Ghosh and Meeden 1986, and Prasad and Rao 1990, Ver Hoef 2008). 

It is assumed that $\mathbf{z}$ is a realization of a spatial stochastic process. Geostatistical models and methods are used (for a review, see Cressie, 1993). Geostatistics has been developed for point samples. If the samples are very small, an infinite population is assumed. The average value over any aggregated area can be predicted using methods such as block kriging. Thus it appears that this is closely related to small area estimation, but where samples come from point locations rather than a finite set of sample units. While there is a large literature on geostatistics and block kriging methods, they have been developed for infinite populations. This package is designed for the case where we have a finite collection of plots and we assume that the data were produced by a spatial stochastic process. Detailed developements are given in Ver Hoef (2008), and we give an overview here.

# References

Ver Hoef, J. M. 2008. Spatial Methods for Plot-Based Sampling of Wildlife Popula
tions. *Environmental and Ecological Statistics* **15**: 3-13.

QUESTIONS: neither data set has area and length of vignette issue

coordinates functions? do we have to?

comparison to other methods....

any other major output function?
